# LLM コスト削減 設計書

## GitHub Issue
https://github.com/k-negishi/ai-curated-newsletter/issues/36

## 現状分析

### コスト構造

現在のコスト推定（`bedrock_cost_estimator.py` 基準、Haiku 4.5 単価）:

| 項目 | 値 |
|------|-----|
| 入力トークン単価 | $1.00 / 1M tokens |
| 出力トークン単価 | $5.00 / 1M tokens |
| 1記事あたり平均入力 | 900 tokens |
| 1記事あたり平均出力 | 140 tokens |
| 1記事あたりコスト | $0.0016 |
| LLM候補上限 | 150件 |
| 1回あたりコスト | ~$0.24 |
| 月額（週2回） | ~$1.92 |

**注**: architecture.md では $0.70/月（週2回 × 120件）と記載されているが、LLM_CANDIDATE_MAX=150 の場合は上記が実コスト。

### コスト削減候補の比較

| アプローチ | 削減効果 | 実装コスト | リスク | 推奨度 |
|-----------|---------|-----------|--------|--------|
| A: LLM候補数の削減 | 中（件数比例） | 低 | ニュースの見落とし | ★★☆ |
| B: 事前フィルタリング強化 | 中〜高 | 中 | フィルタ精度次第 | ★★★ |
| C: プロンプト最適化 | 低（トークン削減） | 低 | 判定品質への影響 | ★★☆ |
| D: バッチ判定（複数記事を1リクエスト） | 高 | 高 | 判定品質低下リスク | ★☆☆ |

## 設計方針

### 採用するアプローチ: B（事前フィルタリング強化）+ C（プロンプト最適化）

**理由**:
- **B**: LLM に送る前に明らかに不要な記事を除外することで、LLM呼び出し回数自体を削減できる。ニュースの見落としリスクが低い
- **C**: 入力トークン数を削減することで1記事あたりのコストを下げる。判定品質に影響しない範囲で実施

### B: 事前フィルタリング強化の設計

現在の処理フロー:
```
RSS収集 → 重複除去(キャッシュ) → LLM候補選定(最大150件) → LLM判定 → 最終選定(15件)
```

改善後:
```
RSS収集 → 重複除去(キャッシュ) → キーワードフィルタ(新規) → LLM候補選定 → LLM判定 → 最終選定
```

**キーワードフィルタの設計**:
- 明らかにスコープ外の記事をタイトル・説明文から除外
- 除外キーワードを `config/sources.yaml` または環境変数で設定可能にする
- フィルタリングされた記事はログに記録（透明性確保）

### C: プロンプト最適化の設計

現在のプロンプトを分析し、以下を検討:
- 不要な指示やコンテキストの削除
- 出力フォーマットの簡素化（不要なフィールドの削除）
- システムプロンプトの共通部分を定数化

## 実装対象ファイル

| ファイル | 変更内容 |
|---------|---------|
| `src/services/` | 事前フィルタリングサービス（新規 or 既存拡張） |
| `src/services/llm_judge.py` | プロンプト最適化 |
| `config/sources.yaml` | フィルタキーワード設定 |
| `src/shared/utils/bedrock_cost_estimator.py` | 必要に応じてパラメータ更新 |
| `docs/architecture.md` | コスト推定値の更新 |

## 注意事項

- コスト削減は判定品質を犠牲にしないことが前提
- 変更前後でコスト推定値を比較し、削減効果を定量的に示す
- 事前フィルタリングの除外ログを残し、誤除外を検知できるようにする
